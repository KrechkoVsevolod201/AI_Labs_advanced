# Лабораторная работа №3: Трансформеры

Выполнена 3 лабораторная работа по изучению архитектуры Transformer и её применению для задач NLP.

Изначально планировал выполнять работу локально, но столкнулся с рядом проблем:
- Сложности со скачиванием IMDb датасета из-за блокировок и нестабильного интернет-соединения
- Конфликты версий библиотек (особенно torch, transformers и accelerate), которые приводили к ошибкам вроде "Some weights of BertForSequenceClassification were not initialized"
- Проблемы с CUDA при попытке использовать GPU локально

Решил перенести работу в Google Colab, что значительно упростило процесс:
- Все необходимые библиотеки уже предустановлены с совместимыми версиями
- Стабильный доступ к датасетам HuggingFace
- Готовая GPU-инфраструктура без дополнительной настройки

В ходе работы использовалась среда со следующими параметрами:
- Python 3.12.3
- PyTorch 2.9.0+cu126
- Transformers 4.57.2
- Accelerate 1.12.0
- NVIDIA CUDA 12.6

Пришлось внести несколько исправлений в код:
- Заменить устаревший параметр `evaluation_strategy` на `eval_strategy`
- Добавить параметр `ignore_mismatched_sizes=True` при загрузке предобученной модели BERT
- Изменить подход к балансировке данных, так как оригинальный IMDb оказался слишком большим для быстрого обучения в Colab

Результаты показали, что BERT (accuracy 0.9000) незначительно превосходит baseline-модели (0.8890) на большой подвыборке данных,
и демонстрирует лучшие показатели F1-меры и более устойчиво работает с несбалансированными данными. Это подтверждает преимущество трансформеров для сложных NLP-задач при достаточном объеме данных и вычислительных ресурсах.
